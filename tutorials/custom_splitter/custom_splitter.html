

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Build your own training/testing split &mdash; lightwood 22.5.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/mindsdblogo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                22.5.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Tutorials</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">API</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Data</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../encoder.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Encoders</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mixer.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Mixers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ensemble.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Ensemble</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analysis.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Analysis</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../helpers.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Helpers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lightwood_philosophy.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Lightwood</span> <span class="pre">Philosophy</span></code></a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">lightwood</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Build your own training/testing split</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/tutorials/custom_splitter/custom_splitter.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Build-your-own-training/testing-split">
<h1>Build your own training/testing split<a class="headerlink" href="#Build-your-own-training/testing-split" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Date:-2021.10.07">
<h2>Date: 2021.10.07<a class="headerlink" href="#Date:-2021.10.07" title="Permalink to this headline">¶</a></h2>
<p>When working with machine learning data, splitting into a “train”, “dev” (or validation) and “test”) set is important. Models use <strong>train</strong> data to learn representations and update their parameters; <strong>dev</strong> or validation data is reserved to see how the model may perform on unknown predictions. While it may not be explicitly trained on, it can be used as a stopping criteria, for hyper-parameter tuning, or as a simple sanity check. Lastly, <strong>test</strong> data is always reserved, hidden from the model,
as a final pass to see what models perform best.</p>
<p>Lightwood supports a variety of <strong>encoders</strong> (Feature engineering procedures) and <strong>mixers</strong> (predictor algorithms that go from feature vectors to the target). Given the diversity of algorithms, it is appropriate to split data into these three categories when <em>preparing</em> encoders or <em>fitting</em> mixers.</p>
<p>Our default approach stratifies labeled data to ensure your train, validation, and test sets are equally represented in all classes. However, in many instances you may want a custom technique to build your own splits. We’ve included the <code class="docutils literal notranslate"><span class="pre">splitter</span></code> functionality (default found in <code class="docutils literal notranslate"><span class="pre">lightwood.data.splitter</span></code>) to enable you to build your own.</p>
<p>In the following problem, we shall work with a Kaggle dataset around credit card fraud (found <a class="reference external" href="https://www.kaggle.com/mlg-ulb/creditcardfraud">here</a>). Fraud detection is difficult because the events we are interested in catching are thankfully rare events. Because of that, there is a large <strong>imbalance of classes</strong> (in fact, in this dataset, less than 1% of the data are the rare-event).</p>
<p>In a supervised technique, we may want to ensure our training data sees the rare event of interest. A random shuffle could potentially miss rare events. We will implement <strong>SMOTE</strong> to increase the number of positive classes in our training data.</p>
<p>Let’s get started!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c1"># Lightwood modules</span>
<span class="kn">import</span> <span class="nn">lightwood</span> <span class="k">as</span> <span class="nn">lw</span>
<span class="kn">from</span> <span class="nn">lightwood</span> <span class="kn">import</span> <span class="n">ProblemDefinition</span><span class="p">,</span> \
                      <span class="n">JsonAI</span><span class="p">,</span> \
                      <span class="n">json_ai_from_problem</span><span class="p">,</span> \
                      <span class="n">code_from_json_ai</span><span class="p">,</span> \
                      <span class="n">predictor_from_code</span>

<span class="kn">import</span> <span class="nn">imblearn</span> <span class="c1"># Vers 0.5.0 minimum requirement</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[nltk_data] Downloading package punkt to /home/runner/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
<span class="ansi-green-fg">INFO:lightwood-2264:No torchvision detected, image helpers not supported.</span>
<span class="ansi-green-fg">INFO:lightwood-2264:No torchvision/pillow detected, image encoder not supported</span>
</pre></div></div>
</div>
<div class="section" id="1)-Load-your-data">
<h3>1) Load your data<a class="headerlink" href="#1)-Load-your-data" title="Permalink to this headline">¶</a></h3>
<p>Lightwood works with <code class="docutils literal notranslate"><span class="pre">pandas</span></code> DataFrames. We can use pandas to load our data. Please download the dataset from the above link and place it in a folder called <code class="docutils literal notranslate"><span class="pre">data/</span></code> where this notebook is located.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://mindsdb-example-data.s3.eu-west-2.amazonaws.com/jupyter/creditcard.csv.zip&quot;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>...</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>0.363787</td>
      <td>...</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>149.62</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>-0.255425</td>
      <td>...</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>2.69</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>-1.514654</td>
      <td>...</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>378.66</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>-1.387024</td>
      <td>...</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>123.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.0</td>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>0.817739</td>
      <td>...</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>69.99</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div></div>
</div>
<p>We see <strong>31 columns</strong>, most of these columns appear numerical. Due to confidentiality reasons, the Kaggle dataset mentions that the columns labeled <span class="math notranslate nohighlight">\(V_i\)</span> indicate principle components (PCs) from a PCA analysis of the original data from the credit card company. There is also a “Time” and “Amount”, two original features that remained. The time references time after the first transaction in the dataset, and amount is how much money was considered in the transaction.</p>
<p>You can also see a heavy imbalance in the two classes below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">],</span> <span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log Counts&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Distribution of Classes&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Text(0.5, 1.0, &#39;Distribution of Classes&#39;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_custom_splitter_custom_splitter_5_1.png" src="../../_images/tutorials_custom_splitter_custom_splitter_5_1.png" />
</div>
</div>
</div>
<div class="section" id="2)-Create-a-JSON-AI-default-object">
<h3>2) Create a JSON-AI default object<a class="headerlink" href="#2)-Create-a-JSON-AI-default-object" title="Permalink to this headline">¶</a></h3>
<p>We will now create JSON-AI syntax for our problem based on its specifications. We can do so by setting up a <code class="docutils literal notranslate"><span class="pre">ProblemDefinition</span></code>. The <code class="docutils literal notranslate"><span class="pre">ProblemDefinition</span></code> allows us to specify the target, the column we intend to predict, along with other details.</p>
<p>The end goal of JSON-AI is to provide **a set of instructions on how to compile a machine learning pipeline*.</p>
<p>Our target here is called “<strong>Class</strong>”, which indicates “0” for no fraud and “1” for fraud. We’ll generate the JSON-AI with the minimal syntax:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup the problem definition</span>
<span class="n">problem_definition</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="s1">&#39;Class&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Generate the j{ai}son syntax</span>
<span class="n">json_ai</span> <span class="o">=</span> <span class="n">json_ai_from_problem</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">problem_definition</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-2264:Dropping features: []</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Analyzing a sample of 18424</span>
<span class="ansi-green-fg">INFO:lightwood-2264:from a total population of 284807, this is equivalent to 6.5% of your data.</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: Time</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column Time has data type integer</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V1</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V1 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V2</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V2 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V3</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V3 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V4</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V4 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V5</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V5 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V6</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V6 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V7</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V7 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V8</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V8 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V9</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V9 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V10</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V10 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V11</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V11 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V12</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V12 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V13</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V13 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V14</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V14 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V15</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V15 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V16</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V16 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V17</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V17 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V18</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V18 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V19</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V19 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V20</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V20 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V21</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V21 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V22</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V22 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V23</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V23 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V24</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V24 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V25</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V25 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V26</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V26 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V27</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V27 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: V28</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column V28 has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: Amount</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column Amount has data type float</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Infering type for: Class</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Column Class has data type binary</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Starting statistical analysis</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Dropping features: []</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Finished statistical analysis</span>
</pre></div></div>
</div>
<p>Lightwood looks at each of the many columns and indicates they are mostly float, with exception of “<strong>Class</strong>” which is binary.</p>
<p>You can observe the JSON-AI if you run the command <code class="docutils literal notranslate"><span class="pre">print(json_ai.to_json())</span></code>. Given there are many input features, we won’t print it out.</p>
<p>These are the only elements required to get off the ground with JSON-AI. However, we’re interested in making a <em>custom</em> approach. So, let’s make this syntax a file, and introduce our own changes.</p>
</div>
<div class="section" id="3)-Build-your-own-splitter-module">
<h3>3) Build your own splitter module<a class="headerlink" href="#3)-Build-your-own-splitter-module" title="Permalink to this headline">¶</a></h3>
<p>For Lightwood, the goal of a splitter is to intake an initial dataset (pre-processed ideally, although you can run the pre-processor on each DataFrame within the splitter) and return a dictionary with the keys “train”, “test”, and “dev” (at minimum). Subsequent steps of the pipeline expect the keys “train”, “test”, and “dev”, so it’s important you assign datasets to these as necessary.</p>
<p>We’re going to introduce SMOTE sampling in our splitter. SMOTE allows you to quickly learn an approximation to make extra “samples” that mimic the undersampled class.</p>
<p>We will use the package <code class="docutils literal notranslate"><span class="pre">imblearn</span></code> and <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> to quickly create a train/test split and apply SMOTE to our training data only.</p>
<p><strong>NOTE</strong> This is simply an example of things you can do with the splitter; whether SMOTE sampling is ideal for your problem depends on the question you’re trying to answer!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> MyCustomSplitter.py

<span class="kn">from</span> <span class="nn">lightwood.api.dtype</span> <span class="kn">import</span> <span class="n">dtype</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">from</span> <span class="nn">lightwood.api.types</span> <span class="kn">import</span> <span class="n">TimeseriesSettings</span>
<span class="kn">from</span> <span class="nn">lightwood.helpers.log</span> <span class="kn">import</span> <span class="n">log</span>


<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>


<span class="k">def</span> <span class="nf">MySplitter</span><span class="p">(</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
    <span class="n">target</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">pct_train</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
    <span class="n">pct_dev</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Custom splitting function</span>


<span class="sd">    :param data: Input data</span>
<span class="sd">    :param target: Name of the target</span>
<span class="sd">    :param pct_train: Percentage of data reserved for training, taken out of full data</span>
<span class="sd">    :param pct_dev: Percentage of data reserved for dev, taken out of train data</span>
<span class="sd">    :param seed: Random seed for reproducibility</span>

<span class="sd">    :returns: A dictionary containing the keys train, test and dev with their respective data frames.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Shuffle the data</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Split into feature columns + target</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="n">target</span><span class="p">]</span>  <span class="c1"># .values</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>  <span class="c1"># .values</span>

    <span class="c1"># Create a train/test split</span>
    <span class="n">X2</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">pct_train</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X2</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">pct_dev</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y2</span>
    <span class="p">)</span>

    <span class="c1"># Create a SMOTE model and bump up underbalanced class JUST for train data</span>
    <span class="n">SMOTE_model</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">Xtrain_mod</span><span class="p">,</span> <span class="n">ytrain_mod</span> <span class="o">=</span> <span class="n">SMOTE_model</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="n">Xtrain_mod</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="n">ytrain_mod</span>
    <span class="n">X_test</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span>
    <span class="n">X_dev</span><span class="p">[</span><span class="n">target</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_dev</span>

    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="n">Xtrain_mod</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span> <span class="n">X_test</span><span class="p">,</span> <span class="s2">&quot;dev&quot;</span><span class="p">:</span> <span class="n">X_dev</span><span class="p">}</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Writing MyCustomSplitter.py
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="Place-your-custom-module-in-~/lightwood_modules">
<h2>Place your custom module in <code class="docutils literal notranslate"><span class="pre">~/lightwood_modules</span></code><a class="headerlink" href="#Place-your-custom-module-in-~/lightwood_modules" title="Permalink to this headline">¶</a></h2>
<p>We automatically search for custom scripts in your <code class="docutils literal notranslate"><span class="pre">~/lightwood_modules</span></code> path. Place your file there. Later, you’ll see when we autogenerate code, that you can change your import location if you choose.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightwood</span> <span class="kn">import</span> <span class="n">load_custom_module</span>

<span class="n">load_custom_module</span><span class="p">(</span><span class="s1">&#39;MyCustomSplitter.py&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="4)-Introduce-your-custom-splitter-in-JSON-AI">
<h3>4) Introduce your custom splitter in JSON-AI<a class="headerlink" href="#4)-Introduce-your-custom-splitter-in-JSON-AI" title="Permalink to this headline">¶</a></h3>
<p>Now let’s introduce our custom splitter. JSON-AI keeps a lightweight syntax but fills in many default modules (like splitting, cleaning).</p>
<p>For the custom cleaner, we’ll work by editing the “splitter” key. We will change properties within it as follows: (1) “module” - place the name of the function. In our case it will be “MyCustomCleaner.cleaner” (2) “args” - any keyword argument specific to your cleaner’s internals.</p>
<p>This will look as follows:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&quot;splitter&quot;: {
    &quot;module&quot;: &quot;MyCustomSplitter.MySplitter&quot;,
    &quot;args&quot;: {
        &quot;data&quot;: &quot;data&quot;,
        &quot;target&quot;: &quot;$target&quot;,
        &quot;pct_train&quot;: 0.8,
        &quot;pct_dev&quot;: 0.1,
        &quot;seed&quot;: 1
    }
},
</pre></div>
</div>
</div>
<div class="section" id="5)-Generate-Python-code-representing-your-ML-pipeline">
<h3>5) Generate Python code representing your ML pipeline<a class="headerlink" href="#5)-Generate-Python-code-representing-your-ML-pipeline" title="Permalink to this headline">¶</a></h3>
<p>Now we’re ready to load up our custom JSON-AI and generate the predictor code!</p>
<p>We can do this by first reading in our custom json-syntax, and then calling the function <code class="docutils literal notranslate"><span class="pre">code_from_json_ai</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">json_ai</span><span class="o">.</span><span class="n">splitter</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;module&quot;</span><span class="p">:</span> <span class="s2">&quot;MyCustomSplitter.MySplitter&quot;</span><span class="p">,</span>
        <span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span>
            <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="s2">&quot;$target&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pct_train&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
            <span class="s2">&quot;pct_dev&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">1</span>
        <span class="p">}</span>
    <span class="p">}</span>

<span class="c1">#Generate python code that fills in your pipeline</span>
<span class="n">code</span> <span class="o">=</span> <span class="n">code_from_json_ai</span><span class="p">(</span><span class="n">json_ai</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>

<span class="c1"># Save code to a file (Optional)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;custom_splitter_pipeline.py&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
    <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-2264:Unable to import black formatter, predictor code might be a bit ugly.</span>
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
import lightwood
from lightwood import __version__ as lightwood_version
from lightwood.analysis import *
from lightwood.api import *
from lightwood.data import *
from lightwood.encoder import *
from lightwood.ensemble import *
from lightwood.helpers.device import *
from lightwood.helpers.general import *
from lightwood.helpers.log import *
from lightwood.helpers.numeric import *
from lightwood.helpers.imputers import *
from lightwood.helpers.parallelism import *
from lightwood.helpers.seed import *
from lightwood.helpers.text import *
from lightwood.helpers.torch import *
from lightwood.mixer import *
import pandas as pd
from typing import Dict, List, Union
import os
from types import ModuleType
import importlib.machinery
import sys
import time


for import_dir in [
    os.path.join(
        os.path.expanduser(&#34;~/lightwood_modules&#34;), lightwood_version.replace(&#34;.&#34;, &#34;_&#34;)
    ),
    os.path.join(&#34;/etc/lightwood_modules&#34;, lightwood_version.replace(&#34;.&#34;, &#34;_&#34;)),
]:
    if os.path.exists(import_dir) and os.access(import_dir, os.R_OK):
        for file_name in list(os.walk(import_dir))[0][2]:
            if file_name[-3:] != &#34;.py&#34;:
                continue
            mod_name = file_name[:-3]
            loader = importlib.machinery.SourceFileLoader(
                mod_name, os.path.join(import_dir, file_name)
            )
            module = ModuleType(loader.name)
            loader.exec_module(module)
            sys.modules[mod_name] = module
            exec(f&#34;import {mod_name}&#34;)


class Predictor(PredictorInterface):
    target: str
    mixers: List[BaseMixer]
    encoders: Dict[str, BaseEncoder]
    ensemble: BaseEnsemble
    mode: str

    def __init__(self):
        seed(420)
        self.target = &#34;Class&#34;
        self.mode = &#34;inactive&#34;
        self.problem_definition = ProblemDefinition.from_dict(
            {
                &#34;target&#34;: &#34;Class&#34;,
                &#34;pct_invalid&#34;: 2,
                &#34;unbias_target&#34;: True,
                &#34;seconds_per_mixer&#34;: 57024.0,
                &#34;seconds_per_encoder&#34;: None,
                &#34;expected_additional_time&#34;: 68.87509536743164,
                &#34;time_aim&#34;: 259200,
                &#34;target_weights&#34;: None,
                &#34;positive_domain&#34;: False,
                &#34;timeseries_settings&#34;: {
                    &#34;is_timeseries&#34;: False,
                    &#34;order_by&#34;: None,
                    &#34;window&#34;: None,
                    &#34;group_by&#34;: None,
                    &#34;use_previous_target&#34;: True,
                    &#34;horizon&#34;: None,
                    &#34;historical_columns&#34;: None,
                    &#34;target_type&#34;: &#34;&#34;,
                    &#34;allow_incomplete_history&#34;: True,
                    &#34;eval_cold_start&#34;: True,
                    &#34;interval_periods&#34;: [],
                },
                &#34;anomaly_detection&#34;: False,
                &#34;use_default_analysis&#34;: True,
                &#34;ignore_features&#34;: [],
                &#34;fit_on_all&#34;: True,
                &#34;strict_mode&#34;: True,
                &#34;seed_nr&#34;: 420,
            }
        )
        self.accuracy_functions = [&#34;balanced_accuracy_score&#34;]
        self.identifiers = {}
        self.dtype_dict = {
            &#34;Time&#34;: &#34;integer&#34;,
            &#34;V1&#34;: &#34;float&#34;,
            &#34;V2&#34;: &#34;float&#34;,
            &#34;V3&#34;: &#34;float&#34;,
            &#34;V4&#34;: &#34;float&#34;,
            &#34;V5&#34;: &#34;float&#34;,
            &#34;V6&#34;: &#34;float&#34;,
            &#34;V7&#34;: &#34;float&#34;,
            &#34;V8&#34;: &#34;float&#34;,
            &#34;V9&#34;: &#34;float&#34;,
            &#34;V10&#34;: &#34;float&#34;,
            &#34;V11&#34;: &#34;float&#34;,
            &#34;V12&#34;: &#34;float&#34;,
            &#34;V13&#34;: &#34;float&#34;,
            &#34;V14&#34;: &#34;float&#34;,
            &#34;V15&#34;: &#34;float&#34;,
            &#34;V16&#34;: &#34;float&#34;,
            &#34;V17&#34;: &#34;float&#34;,
            &#34;V18&#34;: &#34;float&#34;,
            &#34;V19&#34;: &#34;float&#34;,
            &#34;V20&#34;: &#34;float&#34;,
            &#34;V21&#34;: &#34;float&#34;,
            &#34;V22&#34;: &#34;float&#34;,
            &#34;V23&#34;: &#34;float&#34;,
            &#34;V24&#34;: &#34;float&#34;,
            &#34;V25&#34;: &#34;float&#34;,
            &#34;V26&#34;: &#34;float&#34;,
            &#34;V27&#34;: &#34;float&#34;,
            &#34;V28&#34;: &#34;float&#34;,
            &#34;Amount&#34;: &#34;float&#34;,
            &#34;Class&#34;: &#34;binary&#34;,
        }

        # Any feature-column dependencies
        self.dependencies = {
            &#34;Class&#34;: [],
            &#34;Time&#34;: [],
            &#34;V1&#34;: [],
            &#34;V2&#34;: [],
            &#34;V3&#34;: [],
            &#34;V4&#34;: [],
            &#34;V5&#34;: [],
            &#34;V6&#34;: [],
            &#34;V7&#34;: [],
            &#34;V8&#34;: [],
            &#34;V9&#34;: [],
            &#34;V10&#34;: [],
            &#34;V11&#34;: [],
            &#34;V12&#34;: [],
            &#34;V13&#34;: [],
            &#34;V14&#34;: [],
            &#34;V15&#34;: [],
            &#34;V16&#34;: [],
            &#34;V17&#34;: [],
            &#34;V18&#34;: [],
            &#34;V19&#34;: [],
            &#34;V20&#34;: [],
            &#34;V21&#34;: [],
            &#34;V22&#34;: [],
            &#34;V23&#34;: [],
            &#34;V24&#34;: [],
            &#34;V25&#34;: [],
            &#34;V26&#34;: [],
            &#34;V27&#34;: [],
            &#34;V28&#34;: [],
            &#34;Amount&#34;: [],
        }

        self.input_cols = [
            &#34;Time&#34;,
            &#34;V1&#34;,
            &#34;V2&#34;,
            &#34;V3&#34;,
            &#34;V4&#34;,
            &#34;V5&#34;,
            &#34;V6&#34;,
            &#34;V7&#34;,
            &#34;V8&#34;,
            &#34;V9&#34;,
            &#34;V10&#34;,
            &#34;V11&#34;,
            &#34;V12&#34;,
            &#34;V13&#34;,
            &#34;V14&#34;,
            &#34;V15&#34;,
            &#34;V16&#34;,
            &#34;V17&#34;,
            &#34;V18&#34;,
            &#34;V19&#34;,
            &#34;V20&#34;,
            &#34;V21&#34;,
            &#34;V22&#34;,
            &#34;V23&#34;,
            &#34;V24&#34;,
            &#34;V25&#34;,
            &#34;V26&#34;,
            &#34;V27&#34;,
            &#34;V28&#34;,
            &#34;Amount&#34;,
        ]

        # Initial stats analysis
        self.statistical_analysis = None
        self.runtime_log = dict()

    @timed
    def analyze_data(self, data: pd.DataFrame) -&gt; None:
        # Perform a statistical analysis on the unprocessed data

        log.info(&#34;Performing statistical analysis on data&#34;)
        self.statistical_analysis = lightwood.data.statistical_analysis(
            data, self.dtype_dict, {}, self.problem_definition
        )

        # Instantiate post-training evaluation
        self.analysis_blocks = [
            ICP(
                fixed_significance=None,
                confidence_normalizer=False,
                positive_domain=self.statistical_analysis.positive_domain,
            ),
            AccStats(deps=[&#34;ICP&#34;]),
            ConfStats(deps=[&#34;ICP&#34;]),
        ]

    @timed
    def preprocess(self, data: pd.DataFrame) -&gt; pd.DataFrame:
        # Preprocess and clean data

        log.info(&#34;Cleaning the data&#34;)
        self.imputers = {}
        data = cleaner(
            data=data,
            pct_invalid=self.problem_definition.pct_invalid,
            identifiers=self.identifiers,
            dtype_dict=self.dtype_dict,
            target=self.target,
            mode=self.mode,
            imputers=self.imputers,
            timeseries_settings=self.problem_definition.timeseries_settings,
            anomaly_detection=self.problem_definition.anomaly_detection,
        )

        # Time-series blocks

        return data

    @timed
    def split(self, data: pd.DataFrame) -&gt; Dict[str, pd.DataFrame]:
        # Split the data into training/testing splits

        log.info(&#34;Splitting the data into train/test&#34;)
        train_test_data = MyCustomSplitter.MySplitter(
            data=data, pct_train=0.8, pct_dev=0.1, seed=1, target=self.target
        )

        return train_test_data

    @timed
    def prepare(self, data: Dict[str, pd.DataFrame]) -&gt; None:
        # Prepare encoders to featurize data

        self.mode = &#34;train&#34;

        if self.statistical_analysis is None:
            raise Exception(&#34;Please run analyze_data first&#34;)

        # Column to encoder mapping
        self.encoders = {
            &#34;Class&#34;: BinaryEncoder(
                is_target=True, target_weights=self.statistical_analysis.target_weights
            ),
            &#34;Time&#34;: NumericEncoder(),
            &#34;V1&#34;: NumericEncoder(),
            &#34;V2&#34;: NumericEncoder(),
            &#34;V3&#34;: NumericEncoder(),
            &#34;V4&#34;: NumericEncoder(),
            &#34;V5&#34;: NumericEncoder(),
            &#34;V6&#34;: NumericEncoder(),
            &#34;V7&#34;: NumericEncoder(),
            &#34;V8&#34;: NumericEncoder(),
            &#34;V9&#34;: NumericEncoder(),
            &#34;V10&#34;: NumericEncoder(),
            &#34;V11&#34;: NumericEncoder(),
            &#34;V12&#34;: NumericEncoder(),
            &#34;V13&#34;: NumericEncoder(),
            &#34;V14&#34;: NumericEncoder(),
            &#34;V15&#34;: NumericEncoder(),
            &#34;V16&#34;: NumericEncoder(),
            &#34;V17&#34;: NumericEncoder(),
            &#34;V18&#34;: NumericEncoder(),
            &#34;V19&#34;: NumericEncoder(),
            &#34;V20&#34;: NumericEncoder(),
            &#34;V21&#34;: NumericEncoder(),
            &#34;V22&#34;: NumericEncoder(),
            &#34;V23&#34;: NumericEncoder(),
            &#34;V24&#34;: NumericEncoder(),
            &#34;V25&#34;: NumericEncoder(),
            &#34;V26&#34;: NumericEncoder(),
            &#34;V27&#34;: NumericEncoder(),
            &#34;V28&#34;: NumericEncoder(),
            &#34;Amount&#34;: NumericEncoder(),
        }

        # Prepare the training + dev data
        concatenated_train_dev = pd.concat([data[&#34;train&#34;], data[&#34;dev&#34;]])

        log.info(&#34;Preparing the encoders&#34;)

        encoder_prepping_dict = {}

        # Prepare encoders that do not require learned strategies
        for col_name, encoder in self.encoders.items():
            if col_name != self.target and not encoder.is_trainable_encoder:
                encoder_prepping_dict[col_name] = [
                    encoder,
                    concatenated_train_dev[col_name],
                    &#34;prepare&#34;,
                ]
                log.info(
                    f&#34;Encoder prepping dict length of: {len(encoder_prepping_dict)}&#34;
                )

        # Setup parallelization
        parallel_prepped_encoders = mut_method_call(encoder_prepping_dict)
        for col_name, encoder in parallel_prepped_encoders.items():
            self.encoders[col_name] = encoder

        # Prepare the target
        if self.target not in parallel_prepped_encoders:
            if self.encoders[self.target].is_trainable_encoder:
                self.encoders[self.target].prepare(
                    data[&#34;train&#34;][self.target], data[&#34;dev&#34;][self.target]
                )
            else:
                self.encoders[self.target].prepare(
                    pd.concat([data[&#34;train&#34;], data[&#34;dev&#34;]])[self.target]
                )

        # Prepare any non-target encoders that are learned
        for col_name, encoder in self.encoders.items():
            if col_name != self.target and encoder.is_trainable_encoder:
                priming_data = pd.concat([data[&#34;train&#34;], data[&#34;dev&#34;]])
                kwargs = {}
                if self.dependencies[col_name]:
                    kwargs[&#34;dependency_data&#34;] = {}
                    for col in self.dependencies[col_name]:
                        kwargs[&#34;dependency_data&#34;][col] = {
                            &#34;original_type&#34;: self.dtype_dict[col],
                            &#34;data&#34;: priming_data[col],
                        }

                # If an encoder representation requires the target, provide priming data
                if hasattr(encoder, &#34;uses_target&#34;):
                    kwargs[&#34;encoded_target_values&#34;] = self.encoders[self.target].encode(
                        priming_data[self.target]
                    )

                encoder.prepare(
                    data[&#34;train&#34;][col_name], data[&#34;dev&#34;][col_name], **kwargs
                )

    @timed
    def featurize(self, split_data: Dict[str, pd.DataFrame]):
        # Featurize data into numerical representations for models

        log.info(&#34;Featurizing the data&#34;)

        feature_data = {
            key: EncodedDs(self.encoders, data, self.target)
            for key, data in split_data.items()
            if key != &#34;stratified_on&#34;
        }

        return feature_data

    @timed
    def fit(self, enc_data: Dict[str, pd.DataFrame]) -&gt; None:
        # Fit predictors to estimate target

        self.mode = &#34;train&#34;

        # --------------- #
        # Extract data
        # --------------- #
        # Extract the featurized data into train/dev/test
        encoded_train_data = enc_data[&#34;train&#34;]
        encoded_dev_data = enc_data[&#34;dev&#34;]
        encoded_test_data = enc_data[&#34;test&#34;]

        log.info(&#34;Training the mixers&#34;)

        # --------------- #
        # Fit Models
        # --------------- #
        # Assign list of mixers
        self.mixers = [
            Neural(
                fit_on_dev=True,
                search_hyperparameters=True,
                net=&#34;DefaultNet&#34;,
                stop_after=self.problem_definition.seconds_per_mixer,
                target_encoder=self.encoders[self.target],
                target=self.target,
                dtype_dict=self.dtype_dict,
                timeseries_settings=self.problem_definition.timeseries_settings,
            ),
            LightGBM(
                fit_on_dev=True,
                use_optuna=True,
                stop_after=self.problem_definition.seconds_per_mixer,
                target=self.target,
                dtype_dict=self.dtype_dict,
                input_cols=self.input_cols,
                target_encoder=self.encoders[self.target],
            ),
            Regression(
                stop_after=self.problem_definition.seconds_per_mixer,
                target=self.target,
                dtype_dict=self.dtype_dict,
                target_encoder=self.encoders[self.target],
            ),
        ]

        # Train mixers
        trained_mixers = []
        for mixer in self.mixers:
            try:
                self.fit_mixer(mixer, encoded_train_data, encoded_dev_data)
                trained_mixers.append(mixer)
            except Exception as e:
                log.warning(f&#34;Exception: {e} when training mixer: {mixer}&#34;)
                if True and mixer.stable:
                    raise e

        # Update mixers to trained versions
        self.mixers = trained_mixers

        # --------------- #
        # Create Ensembles
        # --------------- #
        log.info(&#34;Ensembling the mixer&#34;)
        # Create an ensemble of mixers to identify best performing model
        self.pred_args = PredictionArguments()
        # Dirty hack
        self.ensemble = BestOf(
            ts_analysis=None,
            data=encoded_test_data,
            args=self.pred_args,
            accuracy_functions=self.accuracy_functions,
            target=self.target,
            mixers=self.mixers,
        )
        self.supports_proba = self.ensemble.supports_proba

    @timed
    def fit_mixer(self, mixer, encoded_train_data, encoded_dev_data) -&gt; None:
        mixer.fit(encoded_train_data, encoded_dev_data)

    @timed
    def analyze_ensemble(self, enc_data: Dict[str, pd.DataFrame]) -&gt; None:
        # Evaluate quality of fit for the ensemble of mixers

        # --------------- #
        # Extract data
        # --------------- #
        # Extract the featurized data into train/dev/test
        encoded_train_data = enc_data[&#34;train&#34;]
        encoded_dev_data = enc_data[&#34;dev&#34;]
        encoded_test_data = enc_data[&#34;test&#34;]

        # --------------- #
        # Analyze Ensembles
        # --------------- #
        log.info(&#34;Analyzing the ensemble of mixers&#34;)
        self.model_analysis, self.runtime_analyzer = model_analyzer(
            data=encoded_test_data,
            train_data=encoded_train_data,
            ts_analysis=None,
            stats_info=self.statistical_analysis,
            tss=self.problem_definition.timeseries_settings,
            accuracy_functions=self.accuracy_functions,
            predictor=self.ensemble,
            target=self.target,
            dtype_dict=self.dtype_dict,
            analysis_blocks=self.analysis_blocks,
        )

    @timed
    def learn(self, data: pd.DataFrame) -&gt; None:
        log.info(f&#34;Dropping features: {self.problem_definition.ignore_features}&#34;)
        data = data.drop(
            columns=self.problem_definition.ignore_features, errors=&#34;ignore&#34;
        )

        self.mode = &#34;train&#34;

        # Perform stats analysis
        self.analyze_data(data)

        # Pre-process the data
        data = self.preprocess(data)

        # Create train/test (dev) split
        train_dev_test = self.split(data)

        # Prepare encoders
        self.prepare(train_dev_test)

        # Create feature vectors from data
        enc_train_test = self.featurize(train_dev_test)

        # Prepare mixers
        self.fit(enc_train_test)

        # Analyze the ensemble
        self.analyze_ensemble(enc_train_test)

        # ------------------------ #
        # Enable model partial fit AFTER it is trained and evaluated for performance with the appropriate train/dev/test splits.
        # This assumes the predictor could continuously evolve, hence including reserved testing data may improve predictions.
        # SET `json_ai.problem_definition.fit_on_all=False` TO TURN THIS BLOCK OFF.

        # Update the mixers with partial fit
        if self.problem_definition.fit_on_all:

            log.info(&#34;Adjustment on validation requested.&#34;)
            self.adjust(
                enc_train_test[&#34;test&#34;],
                ConcatedEncodedDs([enc_train_test[&#34;train&#34;], enc_train_test[&#34;dev&#34;]]),
            )

    @timed
    def adjust(
        self,
        new_data: Union[EncodedDs, ConcatedEncodedDs, pd.DataFrame],
        old_data: Optional[Union[EncodedDs, ConcatedEncodedDs, pd.DataFrame]] = None,
    ) -&gt; None:
        # Update mixers with new information

        self.mode = &#34;train&#34;

        # --------------- #
        # Prepare data
        # --------------- #
        if old_data is None:
            old_data = pd.DataFrame()

        if isinstance(old_data, pd.DataFrame):
            old_data = EncodedDs(self.encoders, old_data, self.target)

        if isinstance(new_data, pd.DataFrame):
            new_data = EncodedDs(self.encoders, new_data, self.target)

        # --------------- #
        # Update/Adjust Mixers
        # --------------- #
        log.info(&#34;Updating the mixers&#34;)

        for mixer in self.mixers:
            mixer.partial_fit(new_data, old_data)

    @timed
    def predict(self, data: pd.DataFrame, args: Dict = {}) -&gt; pd.DataFrame:

        self.mode = &#34;predict&#34;

        if len(data) == 0:
            raise Exception(
                &#34;Empty input, aborting prediction. Please try again with some input data.&#34;
            )

        # Remove columns that user specifies to ignore
        log.info(f&#34;Dropping features: {self.problem_definition.ignore_features}&#34;)
        data = data.drop(
            columns=self.problem_definition.ignore_features, errors=&#34;ignore&#34;
        )
        for col in self.input_cols:
            if col not in data.columns:
                data[col] = [None] * len(data)

        # Pre-process the data
        data = self.preprocess(data)

        # Featurize the data
        encoded_ds = self.featurize({&#34;predict_data&#34;: data})[&#34;predict_data&#34;]
        encoded_data = encoded_ds.get_encoded_data(include_target=False)

        self.pred_args = PredictionArguments.from_dict(args)
        df = self.ensemble(encoded_ds, args=self.pred_args)

        if self.pred_args.all_mixers:
            return df
        else:
            insights, global_insights = explain(
                data=data,
                encoded_data=encoded_data,
                predictions=df,
                ts_analysis=None,
                timeseries_settings=self.problem_definition.timeseries_settings,
                positive_domain=self.statistical_analysis.positive_domain,
                anomaly_detection=self.problem_definition.anomaly_detection,
                analysis=self.runtime_analyzer,
                target_name=self.target,
                target_dtype=self.dtype_dict[self.target],
                explainer_blocks=self.analysis_blocks,
                pred_args=self.pred_args,
            )
            return insights

</pre></div></div>
</div>
<p>As you can see, an end-to-end pipeline of our entire ML procedure has been generating. There are several abstracted functions to enable transparency as to what processes your data goes through in order to build these models.</p>
<p>The key steps of the pipeline are as follows:</p>
<ol class="arabic simple">
<li><p>Run a <strong>statistical analysis</strong> with <code class="docutils literal notranslate"><span class="pre">analyze_data</span></code></p></li>
<li><p>Clean your data with <code class="docutils literal notranslate"><span class="pre">preprocess</span></code></p></li>
<li><p>Make a training/dev/testing split with <code class="docutils literal notranslate"><span class="pre">split</span></code></p></li>
<li><p>Prepare your feature-engineering pipelines with <code class="docutils literal notranslate"><span class="pre">prepare</span></code></p></li>
<li><p>Create your features with <code class="docutils literal notranslate"><span class="pre">featurize</span></code></p></li>
<li><p>Fit your predictor models with <code class="docutils literal notranslate"><span class="pre">fit</span></code></p></li>
</ol>
<p>You can customize this further if necessary, but you have all the steps necessary to train a model!</p>
<p>We recommend familiarizing with these steps by calling the above commands, ideally in order. Some commands (namely <code class="docutils literal notranslate"><span class="pre">prepare</span></code>, <code class="docutils literal notranslate"><span class="pre">featurize</span></code>, and <code class="docutils literal notranslate"><span class="pre">fit</span></code>) do depend on other steps.</p>
<p>If you want to omit the individual steps, we recommend your simply call the <code class="docutils literal notranslate"><span class="pre">learn</span></code> method, which compiles all the necessary steps implemented to give your fully trained predictive models starting with unprocessed data!</p>
</div>
<div class="section" id="6)-Call-python-to-run-your-code-and-see-your-preprocessed-outputs">
<h3>6) Call python to run your code and see your preprocessed outputs<a class="headerlink" href="#6)-Call-python-to-run-your-code-and-see-your-preprocessed-outputs" title="Permalink to this headline">¶</a></h3>
<p>Once we have code, we can turn this into a python object by calling <code class="docutils literal notranslate"><span class="pre">predictor_from_code</span></code>. This instantiates the <code class="docutils literal notranslate"><span class="pre">PredictorInterface</span></code> object.</p>
<p>This predictor object can be then used to run your pipeline.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turn the code above into a predictor object</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">predictor_from_code</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pre-process the data</span>
<span class="n">cleaned_data</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">train_test_data</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">cleaned_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-2264:Cleaning the data</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Dropping features: []</span>
<span class="ansi-white-fg">DEBUG:lightwood-2264: `preprocess` runtime: 14.3 seconds</span>
<span class="ansi-green-fg">INFO:lightwood-2264:Splitting the data into train/test</span>
<span class="ansi-white-fg">DEBUG:lightwood-2264: `split` runtime: 2.71 seconds</span>
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span><span class="o">=</span><span class="mi">15</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_test_data</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s1">&#39;Class&#39;</span><span class="p">],</span> <span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log Counts&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Train:</span><span class="se">\n</span><span class="s2">Distribution of Classes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">])</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_test_data</span><span class="p">[</span><span class="s2">&quot;dev&quot;</span><span class="p">][</span><span class="s1">&#39;Class&#39;</span><span class="p">],</span> <span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log Counts&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Dev:</span><span class="se">\n</span><span class="s2">Distribution of Classes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">])</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_test_data</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][</span><span class="s1">&#39;Class&#39;</span><span class="p">],</span> <span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log Counts&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Test:</span><span class="se">\n</span><span class="s2">Distribution of Classes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">])</span>

<span class="n">f</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_custom_splitter_custom_splitter_21_0.png" src="../../_images/tutorials_custom_splitter_custom_splitter_21_0.png" />
</div>
</div>
<p>As you can see, our splitter has greatly increased the representation of the minority class within the training data, but not so for the testing or dev data.</p>
<p>We hope this tutorial was informative on how to introduce a <strong>custom splitter method</strong> to your datasets! For more customization tutorials, please check our <a class="reference external" href="https://lightwood.io/tutorials.html">documentation</a>.</p>
<p>If you want to download the Jupyter-notebook version of this tutorial, check out the source github location found here: <code class="docutils literal notranslate"><span class="pre">lightwood/docssrc/source/tutorials/custom_splitter</span></code>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2017-2022, MindsDB.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>