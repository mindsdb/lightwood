

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial - Implementing a custom analysis block in Lightwood &mdash; lightwood 25.3.3.2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />

  
      <script src="../../_static/jquery.js"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
      <script src="../../_static/doctools.js"></script>
      <script src="../../_static/sphinx_highlight.js"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/mindsdblogo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Tutorials</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">API</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Data</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../encoder.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Encoders</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mixer.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Mixers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ensemble.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Ensemble</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../analysis.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Analysis</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../helpers.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Helpers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lightwood_philosophy.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">Lightwood</span> <span class="pre">Philosophy</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: white" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">lightwood</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Tutorial - Implementing a custom analysis block in Lightwood</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/custom_explainer/custom_explainer.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Tutorial---Implementing-a-custom-analysis-block-in-Lightwood">
<h1>Tutorial - Implementing a custom analysis block in Lightwood<a class="headerlink" href="#Tutorial---Implementing-a-custom-analysis-block-in-Lightwood" title="Permalink to this heading"></a></h1>
<section id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this heading"></a></h2>
<p>As you might already know, Lightwood is designed to be a flexible machine learning (ML) library that is able to abstract and automate the entire ML pipeline. Crucially, it is also designed to be extended or modified very easily according to your needs, essentially offering the entire spectrum between fully automated AutoML and a lightweight wrapper for customized ML pipelines.</p>
<p>As such, we can identify several different customizable “phases” in the process. The relevant phase for this tutorial is the “analysis” that comes after a predictor has been trained. The goal of this phase is to generate useful insights, like accuracy metrics, confusion matrices, feature importance, etc. These particular examples are all included in the core analysis procedure that Lightwood executes.</p>
<p>However, the analysis procedure is structured into a sequential execution of “analysis blocks”. Each analysis block should generate a well-defined set of insights, as well as handling any actions regarding these at inference time.</p>
<p>As an example, one of the core blocks is the Inductive Conformal Prediction (<code class="docutils literal notranslate"><span class="pre">ICP</span></code>) block, which handles the confidence estimation of all Lightwood predictors. The logic within can be complex at times, but thanks to the block abstraction we can deal with it in a structured manner. As this <code class="docutils literal notranslate"><span class="pre">ICP</span></code> block is used when generating predictions, it implements the two main methods that the <code class="docutils literal notranslate"><span class="pre">BaseAnalysisBlock</span></code> class specifies: <code class="docutils literal notranslate"><span class="pre">.analyze()</span></code> to setup everything that is needed, and <code class="docutils literal notranslate"><span class="pre">.explain()</span></code> to
actually estimate the confidence in any given prediction.</p>
</section>
<section id="Objective">
<h2>Objective<a class="headerlink" href="#Objective" title="Permalink to this heading"></a></h2>
<p>In this tutorial, we will go through the steps required to implement your own analysis blocks to customize the insights of any Lightwood predictor!</p>
<p>In particular, we will implement a “model correlation heatmap” block: we want to compare the predictions of all mixers inside a <code class="docutils literal notranslate"><span class="pre">BestOf</span></code> ensemble object, to understand how they might differ in their overall behavior.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">lightwood</span>
<span class="n">lightwood</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-3286:No torchvision detected, image helpers not supported.</span>
<span class="ansi-green-fg">INFO:lightwood-3286:No torchvision/pillow detected, image encoder not supported</span>
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;25.3.3.2&#39;
</pre></div></div>
</div>
</section>
<section id="Step-1:-figuring-out-what-we-need">
<h2>Step 1: figuring out what we need<a class="headerlink" href="#Step-1:-figuring-out-what-we-need" title="Permalink to this heading"></a></h2>
<p>When designing an analysis block, an important choice needs to be made: will this block operate when calling the predictor? Or is it only going to describe its performance once in the held-out validation dataset?</p>
<p>Being in the former case means we need to implement both <code class="docutils literal notranslate"><span class="pre">.analyze()</span></code> and <code class="docutils literal notranslate"><span class="pre">.explain()</span></code> methods, while the latter case only needs an <code class="docutils literal notranslate"><span class="pre">.analyze()</span></code> method. Our <code class="docutils literal notranslate"><span class="pre">ModelCorrelationHeatmap</span></code> belongs to this second category.</p>
<p>Let’s start the implementation by inheriting from <code class="docutils literal notranslate"><span class="pre">BaseAnalysisBlock</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">lightwood.analysis</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseAnalysisBlock</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ModelCorrelationHeatmap</span><span class="p">(</span><span class="n">BaseAnalysisBlock</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deps</span><span class="o">=</span><span class="nb">tuple</span><span class="p">()):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">deps</span><span class="o">=</span><span class="n">deps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">analyze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">info</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">explain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">row_insights</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                <span class="n">global_insights</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]]:</span>

        <span class="k">return</span> <span class="n">row_insights</span><span class="p">,</span> <span class="n">global_insights</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ModelCorrelationHeatmap</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;__main__.ModelCorrelationHeatmap at 0x7f2971bb6790&gt;
</pre></div></div>
</div>
<p>Right now, our newly created analysis block doesn’t do much, apart from returning the <code class="docutils literal notranslate"><span class="pre">info</span></code> and insights (<code class="docutils literal notranslate"><span class="pre">row_insights</span></code> and <code class="docutils literal notranslate"><span class="pre">global_insights</span></code>) exactly as it received them from the previous block.</p>
<p>As previously discussed, we only need to implement a procedure that runs post-training, no action is required at inference time. This means we can use the default <code class="docutils literal notranslate"><span class="pre">.explain()</span></code> behavior in the parent class:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ModelCorrelationHeatmap</span><span class="p">(</span><span class="n">BaseAnalysisBlock</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deps</span><span class="o">=</span><span class="nb">tuple</span><span class="p">()):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">deps</span><span class="o">=</span><span class="n">deps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">analyze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">info</span>
</pre></div>
</div>
</div>
</section>
<section id="Step-2:-Implementing-the-custom-analysis-block">
<h2>Step 2: Implementing the custom analysis block<a class="headerlink" href="#Step-2:-Implementing-the-custom-analysis-block" title="Permalink to this heading"></a></h2>
<p>Okay, now for the fun bit: we have to implement a correlation heatmap between the predictions of all mixers inside a <code class="docutils literal notranslate"><span class="pre">BestOf</span></code> ensemble. This is currently the only ensemble implemented in Lightwood, but it is a good idea to explicitly check that the type of the ensemble is what we expect.</p>
<p>A natural question to ask at this point is: what information do we have to implement the procedure? You’ll note that, apart from the <code class="docutils literal notranslate"><span class="pre">info</span></code> dictionary, we receive a <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> dictionary. You can check out the full documentation for more details, but the keys (and respective value types) exposed in this object by default are:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;predictor&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.ensemble.BaseEnsemble&#39;</span><span class="p">,</span>
        <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="s1">&#39;str&#39;</span><span class="p">,</span>
        <span class="s1">&#39;input_cols&#39;</span><span class="p">:</span> <span class="s1">&#39;list&#39;</span><span class="p">,</span>
        <span class="s1">&#39;dtype_dict&#39;</span><span class="p">:</span> <span class="s1">&#39;dict&#39;</span><span class="p">,</span>
        <span class="s1">&#39;normal_predictions&#39;</span><span class="p">:</span> <span class="s1">&#39;pd.DataFrame&#39;</span><span class="p">,</span>
        <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="s1">&#39;pd.DataFrame&#39;</span><span class="p">,</span>
        <span class="s1">&#39;train_data&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.data.encoded_ds.EncodedDs&#39;</span><span class="p">,</span>
        <span class="s1">&#39;encoded_val_data&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.data.encoded_ds.EncodedDs&#39;</span><span class="p">,</span>
        <span class="s1">&#39;is_classification&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span><span class="p">,</span>
        <span class="s1">&#39;is_numerical&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span><span class="p">,</span>
        <span class="s1">&#39;is_multi_ts&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span><span class="p">,</span>
        <span class="s1">&#39;stats_info&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.api.types.StatisticalAnalysis&#39;</span><span class="p">,</span>
        <span class="s1">&#39;ts_cfg&#39;</span><span class="p">:</span> <span class="s1">&#39;lightwood.api.types.TimeseriesSettings&#39;</span><span class="p">,</span>
        <span class="s1">&#39;accuracy_functions&#39;</span><span class="p">:</span> <span class="s1">&#39;list&#39;</span><span class="p">,</span>
        <span class="s1">&#39;has_pretrained_text_enc&#39;</span><span class="p">:</span> <span class="s1">&#39;bool&#39;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<p>As you can see there is lots to work with, but for this example we will focus on using:</p>
<ol class="arabic simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">predictor</span></code> ensemble</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">encoded_val_data</span></code> to generate predictions for each mixer inside the ensemble</p></li>
</ol>
<p>And the insight we’re want to produce is a matrix that compares the output of all mixers and computes the correlation between them.</p>
<p>Let’s implement the algorithm:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> model_correlation.py

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">types</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleNamespace</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">lightwood.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">BestOf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lightwood.analysis</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseAnalysisBlock</span>


<span class="k">class</span><span class="w"> </span><span class="nc">ModelCorrelationHeatmap</span><span class="p">(</span><span class="n">BaseAnalysisBlock</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deps</span><span class="o">=</span><span class="nb">tuple</span><span class="p">()):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">deps</span><span class="o">=</span><span class="n">deps</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">analyze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">info</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>
        <span class="n">ns</span> <span class="o">=</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># only triggered with the right type of ensemble</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ns</span><span class="o">.</span><span class="n">predictor</span><span class="p">,</span> <span class="n">BestOf</span><span class="p">):</span>

            <span class="c1"># store prediction from every mixer</span>
            <span class="n">all_predictions</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">mixer</span> <span class="ow">in</span> <span class="n">ns</span><span class="o">.</span><span class="n">predictor</span><span class="o">.</span><span class="n">mixers</span><span class="p">:</span>
                <span class="n">predictions</span> <span class="o">=</span> <span class="n">mixer</span><span class="p">(</span><span class="n">ns</span><span class="o">.</span><span class="n">encoded_val_data</span><span class="p">)[</span><span class="s1">&#39;prediction&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># retrieve np.ndarray from the returned pd.DataFrame</span>
                <span class="n">all_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>  <span class="c1"># flatten and cast labels to int</span>

            <span class="c1"># calculate correlation matrix</span>
            <span class="n">corrs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">all_predictions</span><span class="p">))</span>

            <span class="c1"># save inside `info` object</span>
            <span class="n">info</span><span class="p">[</span><span class="s1">&#39;mixer_correlation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">corrs</span>

        <span class="k">return</span> <span class="n">info</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Writing model_correlation.py
</pre></div></div>
</div>
<p>Notice the use of <code class="docutils literal notranslate"><span class="pre">SimpleNamespace</span></code> for dot notation accessors.</p>
<p>The procedure above is fairly straightforward, as we leverage numpy’s <code class="docutils literal notranslate"><span class="pre">corrcoef()</span></code> function to generate the matrix.</p>
<p>Finally, it is very important to add the output to <code class="docutils literal notranslate"><span class="pre">info</span></code> so that it is saved inside the actual predictor object.</p>
</section>
<section id="Step-3:-Exposing-the-block-to-Lightwood">
<h2>Step 3: Exposing the block to Lightwood<a class="headerlink" href="#Step-3:-Exposing-the-block-to-Lightwood" title="Permalink to this heading"></a></h2>
<p>To use this in an arbitrary script, we need to add the above class (and all necessary imports) to a <code class="docutils literal notranslate"><span class="pre">.py</span></code> file inside one of the following directories:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">~/lightwood_modules</span></code> (where <code class="docutils literal notranslate"><span class="pre">~</span></code> is your home directory, e.g. <code class="docutils literal notranslate"><span class="pre">/Users/username/</span></code> for macOS and <code class="docutils literal notranslate"><span class="pre">/home/username/</span></code> for linux</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/etc/lightwood_modules</span></code></p></li>
</ul>
<p>Lightwood will scan these directories and import any class so that they can be found and used by the <code class="docutils literal notranslate"><span class="pre">JsonAI</span></code> code generating module.</p>
<p><strong>To continue, please save the code cell above as ``model_correlation.py`` in one of the indicated directories.</strong></p>
</section>
<section id="Step-4:-Final-test-run">
<h2>Step 4: Final test run<a class="headerlink" href="#Step-4:-Final-test-run" title="Permalink to this heading"></a></h2>
<p>Ok! Everything looks set to try out our custom block. Let’s generate a predictor for <a class="reference external" href="https://github.com/mindsdb/lightwood/blob/stable/tests/data/hdi.csv">this</a> sample dataset, and see whether our new insights are any good.</p>
<p>First, it is important to add our <code class="docutils literal notranslate"><span class="pre">ModelCorrelationHeatmap</span></code> to the <code class="docutils literal notranslate"><span class="pre">analysis_blocks</span></code> attribute of the Json AI object that will generate your predictor code.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">lightwood.api.high_level</span><span class="w"> </span><span class="kn">import</span> <span class="n">ProblemDefinition</span><span class="p">,</span> <span class="n">json_ai_from_problem</span><span class="p">,</span> <span class="n">load_custom_module</span>

<span class="c1"># First, load the custom module we wrote</span>
<span class="n">load_custom_module</span><span class="p">(</span><span class="s1">&#39;model_correlation.py&#39;</span><span class="p">)</span>

<span class="c1"># read dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/mindsdb/lightwood/main/tests/data/hdi.csv&#39;</span><span class="p">)</span>

<span class="c1"># define the predictive task</span>
<span class="n">pdef</span> <span class="o">=</span> <span class="n">ProblemDefinition</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="s1">&#39;Development Index&#39;</span><span class="p">,</span>         <span class="c1"># column you want to predict</span>
    <span class="s1">&#39;time_aim&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
<span class="p">})</span>

<span class="c1"># generate the Json AI intermediate representation from the data and its corresponding settings</span>
<span class="n">json_ai</span> <span class="o">=</span> <span class="n">json_ai_from_problem</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">problem_definition</span><span class="o">=</span><span class="n">pdef</span><span class="p">)</span>

<span class="c1"># add the custom list of analysis blocks; in this case, composed of a single block</span>
<span class="n">json_ai</span><span class="o">.</span><span class="n">analysis_blocks</span> <span class="o">=</span> <span class="p">[{</span>
    <span class="s1">&#39;module&#39;</span><span class="p">:</span> <span class="s1">&#39;model_correlation.ModelCorrelationHeatmap&#39;</span><span class="p">,</span>
    <span class="s1">&#39;args&#39;</span><span class="p">:</span> <span class="p">{}</span>
<span class="p">}]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:type_infer-3286:Analyzing a sample of 222</span>
<span class="ansi-green-fg">INFO:type_infer-3286:from a total population of 225, this is equivalent to 98.7% of your data.</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Infering type for: Population</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Column Population has data type integer</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Infering type for: Area (sq. mi.)</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Column Area (sq. mi.) has data type integer</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Infering type for: Pop. Density </span>
<span class="ansi-green-fg">INFO:type_infer-3286:Column Pop. Density  has data type float</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Infering type for: GDP ($ per capita)</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Column GDP ($ per capita) has data type integer</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Infering type for: Literacy (%)</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Column Literacy (%) has data type float</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Infering type for: Infant mortality </span>
<span class="ansi-green-fg">INFO:type_infer-3286:Column Infant mortality  has data type float</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Infering type for: Development Index</span>
<span class="ansi-green-fg">INFO:type_infer-3286:Column Development Index has data type categorical</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Starting statistical analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Finished statistical analysis</span>
</pre></div></div>
</div>
<p>We can take a look at the respective Json AI key just to confirm our newly added analysis block is in there:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">json_ai</span><span class="o">.</span><span class="n">analysis_blocks</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[{&#39;module&#39;: &#39;model_correlation.ModelCorrelationHeatmap&#39;, &#39;args&#39;: {}}]
</pre></div></div>
</div>
<p>Now we are ready to create a predictor from this Json AI, and subsequently train it:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">lightwood.api.high_level</span><span class="w"> </span><span class="kn">import</span> <span class="n">code_from_json_ai</span><span class="p">,</span> <span class="n">predictor_from_code</span>

<span class="n">code</span> <span class="o">=</span> <span class="n">code_from_json_ai</span><span class="p">(</span><span class="n">json_ai</span><span class="p">)</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">predictor_from_code</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>

<span class="n">predictor</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 1/8] - Statistical analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Starting statistical analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Finished statistical analysis</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `analyze_data` runtime: 0.02 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 2/8] - Data preprocessing</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Cleaning the data</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `preprocess` runtime: 0.01 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 3/8] - Data splitting</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Splitting the data into train/test</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `split` runtime: 0.01 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 4/8] - Preparing encoders</span>
<span class="ansi-white-fg">DEBUG:dataprep_ml-3286:Preparing sequentially...</span>
<span class="ansi-white-fg">DEBUG:dataprep_ml-3286:Preparing encoder for Population...</span>
<span class="ansi-white-fg">DEBUG:dataprep_ml-3286:Preparing encoder for Area (sq. mi.)...</span>
<span class="ansi-white-fg">DEBUG:dataprep_ml-3286:Preparing encoder for Pop. Density ...</span>
<span class="ansi-white-fg">DEBUG:dataprep_ml-3286:Preparing encoder for GDP ($ per capita)...</span>
<span class="ansi-white-fg">DEBUG:dataprep_ml-3286:Preparing encoder for Literacy (%)...</span>
<span class="ansi-white-fg">DEBUG:dataprep_ml-3286:Preparing encoder for Infant mortality ...</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286:Encoding UNKNOWN categories as index 0</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `prepare` runtime: 0.01 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 5/8] - Feature generation</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Featurizing the data</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `featurize` runtime: 0.05 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 6/8] - Mixer training</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Training the mixers</span>
<span class="ansi-yellow-fg">WARNING:lightwood-3286:XGBoost running on CPU</span>
/home/runner/work/lightwood/lightwood/lightwood/mixer/neural.py:124: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler(&#39;cuda&#39;, args...)` instead.
  scaler = GradScaler()
/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:
        addcmul_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
        addcmul_(Tensor tensor1, Tensor tensor2, *, Number value = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1661.)
  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 18.69619858264923 with learning rate 0.0001</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 16.93891429901123 with learning rate 0.0005</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 16.197376608848572 with learning rate 0.001</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 16.06481909751892 with learning rate 0.002</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 16.472004413604736 with learning rate 0.003</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 18.28026556968689 with learning rate 0.005</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 26.746760368347168 with learning rate 0.01</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss of 101.83524441719055 with learning rate 0.05</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Found learning rate of: 0.002</span>
/home/runner/work/lightwood/lightwood/lightwood/mixer/neural.py:305: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler(&#39;cuda&#39;, args...)` instead.
  scaler = GradScaler()
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 1: 1.319209337234497</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 2: 1.3220206499099731</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 3: 1.3063435554504395</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 4: 1.2932535409927368</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 5: 1.2823516130447388</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 6: 1.2705544233322144</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 7: 1.2418551445007324</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 8: 1.2208324670791626</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 9: 1.197828769683838</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 10: 1.1781431436538696</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 11: 1.161504864692688</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 12: 1.1442031860351562</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 13: 1.1058541536331177</span>
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[09:51:55] WARNING: ../src/learner.cc:339: No visible GPU is found, setting `gpu_id` to -1
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 14: 1.0935649871826172</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 15: 1.0802721977233887</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 16: 1.0696258544921875</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 17: 1.0607414245605469</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 18: 1.0493905544281006</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 19: 1.020617961883545</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 20: 1.0081787109375</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 21: 0.9943330883979797</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 22: 0.9842473268508911</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 23: 0.9762145280838013</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 24: 0.9653865098953247</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 25: 0.9380742311477661</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 26: 0.9271669387817383</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 27: 0.9147073030471802</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 28: 0.9064992070198059</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 29: 0.900122344493866</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 30: 0.8903173208236694</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 31: 0.8648637533187866</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 32: 0.8549227118492126</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 33: 0.8434366583824158</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 34: 0.8365358114242554</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 35: 0.831310510635376</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 36: 0.8222441673278809</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 37: 0.7981722950935364</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 38: 0.789170503616333</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 39: 0.7787192463874817</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 40: 0.7730110287666321</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 41: 0.7687097787857056</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 42: 0.7602015137672424</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 43: 0.7373268604278564</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 44: 0.7292225956916809</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 45: 0.7197889685630798</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 46: 0.7151773571968079</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 47: 0.7117206454277039</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 48: 0.7038285136222839</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 49: 0.682073175907135</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 50: 0.674643874168396</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 51: 0.6659626364707947</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 52: 0.6620772480964661</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 53: 0.6590715646743774</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 54: 0.6515910625457764</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 55: 0.6308077573776245</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 56: 0.6241987347602844</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 57: 0.6163835525512695</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 58: 0.6131908297538757</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 59: 0.6106155514717102</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 60: 0.6036757826805115</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 61: 0.5848420262336731</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 62: 0.5793871879577637</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 63: 0.5726662278175354</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 64: 0.5703645348548889</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 65: 0.5684641003608704</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 66: 0.5622180104255676</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 67: 0.5449516773223877</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 68: 0.5401747226715088</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 69: 0.5341063141822815</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 70: 0.5322306752204895</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 71: 0.5305425524711609</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 72: 0.5246548056602478</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 73: 0.5083626508712769</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 74: 0.5040708184242249</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 75: 0.49863335490226746</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 76: 0.49717265367507935</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 77: 0.49564701318740845</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 78: 0.4900944232940674</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 79: 0.47473227977752686</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 80: 0.4708785116672516</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 81: 0.46578508615493774</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 82: 0.4644494950771332</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 83: 0.4629424810409546</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 84: 0.4576236307621002</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 85: 0.44295966625213623</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 86: 0.4393518269062042</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 87: 0.4346559941768646</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 88: 0.43358123302459717</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 89: 0.43231165409088135</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 90: 0.42753666639328003</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 91: 0.41425880789756775</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 92: 0.4113253951072693</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 93: 0.4074642062187195</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 94: 0.40698081254959106</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 95: 0.4062195420265198</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 96: 0.4020974636077881</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 97: 0.3904534876346588</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 98: 0.3879585564136505</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 99: 0.38447296619415283</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 100: 0.3841344118118286</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 101: 0.3833732604980469</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 102: 0.37956172227859497</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 103: 0.36883822083473206</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 104: 0.3664571940898895</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 105: 0.36335548758506775</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 106: 0.36316758394241333</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 107: 0.3625164330005646</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 108: 0.359012246131897</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 109: 0.34912365674972534</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 110: 0.34696850180625916</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 111: 0.3441202938556671</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 112: 0.34398093819618225</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 113: 0.3432472050189972</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 114: 0.3399496376514435</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 115: 0.3308461308479309</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 116: 0.3289041221141815</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 117: 0.32627207040786743</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 118: 0.3261372447013855</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 119: 0.32546231150627136</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 120: 0.3224080502986908</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 121: 0.314008891582489</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 122: 0.31220486760139465</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 123: 0.3098214566707611</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 124: 0.30979809165000916</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 125: 0.3090403079986572</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 126: 0.30612480640411377</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 127: 0.29819032549858093</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 128: 0.29648637771606445</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 129: 0.2943042516708374</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 130: 0.29420480132102966</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 131: 0.2934538424015045</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 132: 0.2907505929470062</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 133: 0.2835786044597626</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 134: 0.28203263878822327</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 135: 0.2801313102245331</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 136: 0.2801584303379059</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 137: 0.27946653962135315</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 138: 0.2770102620124817</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 139: 0.2705138921737671</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 140: 0.2689667046070099</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 141: 0.26713839173316956</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 142: 0.26722976565361023</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 143: 0.26659274101257324</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 144: 0.26436734199523926</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 145: 0.2585783302783966</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 146: 0.25718021392822266</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 147: 0.25569674372673035</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 148: 0.25572293996810913</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 149: 0.254925012588501</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 150: 0.25273722410202026</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 151: 0.24740025401115417</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 152: 0.24594709277153015</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 153: 0.2445458471775055</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 154: 0.2445453554391861</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 155: 0.24368290603160858</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 156: 0.2416052669286728</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 157: 0.23683445155620575</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 158: 0.23561015725135803</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 159: 0.2342897355556488</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 160: 0.2342986762523651</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 161: 0.2334001511335373</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 162: 0.23141320049762726</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 163: 0.22705355286598206</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 164: 0.22583813965320587</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 165: 0.22455710172653198</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 166: 0.2245052307844162</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 167: 0.22359803318977356</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 168: 0.22173909842967987</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 169: 0.2178291231393814</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 170: 0.21670299768447876</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 171: 0.21559178829193115</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 172: 0.21557293832302094</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 173: 0.21463343501091003</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 174: 0.21291333436965942</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 175: 0.20953477919101715</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 176: 0.20840951800346375</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 177: 0.20733794569969177</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 178: 0.20730628073215485</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 179: 0.20635393261909485</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 180: 0.20470596849918365</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 181: 0.2016059160232544</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 182: 0.2004680186510086</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 183: 0.1995442509651184</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 184: 0.1995476931333542</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 185: 0.1985597461462021</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 186: 0.19704405963420868</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 187: 0.19429439306259155</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 188: 0.1931215077638626</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 189: 0.19224728643894196</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 190: 0.1922168731689453</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 191: 0.19120150804519653</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 192: 0.1897118091583252</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 193: 0.187192901968956</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 194: 0.18604235351085663</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 195: 0.18525990843772888</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 196: 0.18517257273197174</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 197: 0.1841844767332077</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 198: 0.18275843560695648</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 199: 0.18052375316619873</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 200: 0.1795222908258438</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 201: 0.17878693342208862</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 202: 0.17881926894187927</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 203: 0.17786364257335663</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 204: 0.17654098570346832</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 205: 0.17458021640777588</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 206: 0.17358489334583282</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 207: 0.1728101521730423</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 208: 0.17278429865837097</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 209: 0.17180292308330536</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 210: 0.17050613462924957</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 211: 0.16873842477798462</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 212: 0.1677248477935791</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 213: 0.16707710921764374</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 214: 0.1671123504638672</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 215: 0.16612616181373596</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 216: 0.16487975418567657</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 217: 0.16339382529258728</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 218: 0.1624278575181961</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 219: 0.16172048449516296</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 220: 0.16165515780448914</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 221: 0.16061937808990479</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 222: 0.1594206690788269</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 223: 0.15802235901355743</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 224: 0.15704363584518433</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 225: 0.15640243887901306</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 226: 0.15635541081428528</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 227: 0.15536457300186157</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 228: 0.154209703207016</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 229: 0.15291643142700195</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 230: 0.15191468596458435</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 231: 0.15118129551410675</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 232: 0.151133194565773</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 233: 0.1501670926809311</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 234: 0.14912192523479462</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 235: 0.1481197327375412</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 236: 0.14712536334991455</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 237: 0.14660944044589996</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 238: 0.14649781584739685</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 239: 0.145524799823761</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 240: 0.14443959295749664</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 241: 0.14341002702713013</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 242: 0.14249812066555023</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 243: 0.14185366034507751</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 244: 0.1419423222541809</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 245: 0.1409326195716858</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 246: 0.1399424970149994</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 247: 0.13920661807060242</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 248: 0.13832959532737732</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 249: 0.13784818351268768</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 250: 0.13769637048244476</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 251: 0.1367887407541275</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 252: 0.13588252663612366</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 253: 0.13521170616149902</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 254: 0.13428467512130737</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 255: 0.13384407758712769</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 256: 0.13372991979122162</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 257: 0.13274942338466644</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 258: 0.13186557590961456</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 259: 0.13135230541229248</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 260: 0.13046588003635406</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 261: 0.12980172038078308</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 262: 0.12982229888439178</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 263: 0.12882845103740692</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 264: 0.12798714637756348</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 265: 0.12758088111877441</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 266: 0.12660843133926392</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 267: 0.1261577606201172</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 268: 0.1260918229818344</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 269: 0.12515920400619507</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 270: 0.12436933070421219</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 271: 0.12405422329902649</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 272: 0.12305409461259842</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 273: 0.12272939085960388</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 274: 0.12267134338617325</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 275: 0.12182944267988205</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 276: 0.12103450298309326</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 277: 0.12083345651626587</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 278: 0.11998103559017181</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 279: 0.11937755346298218</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 280: 0.1195112019777298</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 281: 0.1185888797044754</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 282: 0.11789504438638687</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 283: 0.11783000081777573</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 284: 0.11681754887104034</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 285: 0.11649196594953537</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 286: 0.11648327857255936</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 287: 0.11562418192625046</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 288: 0.11489420384168625</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 289: 0.11485717445611954</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 290: 0.11407709866762161</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 291: 0.11348505318164825</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 292: 0.11358898878097534</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 293: 0.11268813163042068</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 294: 0.11207651346921921</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 295: 0.11220688372850418</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 296: 0.11118005961179733</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 297: 0.11089354008436203</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 298: 0.11088859289884567</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 299: 0.1100316271185875</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 300: 0.1093800961971283</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 301: 0.10955681651830673</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 302: 0.10869839787483215</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 303: 0.10815789550542831</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 304: 0.10832306742668152</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 305: 0.10742544382810593</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 306: 0.10682710260152817</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 307: 0.10698221623897552</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 308: 0.10616409033536911</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 309: 0.10568025708198547</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 310: 0.10574078559875488</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 311: 0.10493995994329453</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 312: 0.10438455641269684</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 313: 0.10449497401714325</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 314: 0.10372297465801239</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 315: 0.10320515185594559</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 316: 0.10330332815647125</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 317: 0.10239537805318832</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 318: 0.10185065120458603</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 319: 0.10217708349227905</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 320: 0.10135672241449356</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 321: 0.10087659955024719</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 322: 0.10087589174509048</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 323: 0.10005565732717514</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 324: 0.09949999302625656</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 325: 0.09970266371965408</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 326: 0.09918338060379028</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 327: 0.09840800613164902</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 328: 0.09882311522960663</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 329: 0.09775345772504807</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 330: 0.09729817509651184</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 331: 0.09763044863939285</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 332: 0.0967596173286438</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 333: 0.09642492234706879</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 334: 0.09656761586666107</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 335: 0.09573261439800262</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 336: 0.09523642063140869</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 337: 0.09568659961223602</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 338: 0.09509280323982239</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 339: 0.09460369497537613</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 340: 0.09476538747549057</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 341: 0.09388881921768188</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 342: 0.09349637478590012</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 343: 0.09398090839385986</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 344: 0.09314301609992981</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 345: 0.09281699359416962</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 346: 0.09290202707052231</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 347: 0.09209518879652023</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 348: 0.09171803295612335</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 349: 0.09221566468477249</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 350: 0.09150414168834686</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 351: 0.0910501629114151</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 352: 0.09118885546922684</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 353: 0.09043896198272705</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 354: 0.09006913751363754</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 355: 0.09049264341592789</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 356: 0.0898597463965416</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 357: 0.08943390846252441</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 358: 0.0896739661693573</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 359: 0.08882326632738113</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 360: 0.08850156515836716</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 361: 0.08897048979997635</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 362: 0.08849596232175827</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 363: 0.08790712803602219</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 364: 0.08821234852075577</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 365: 0.08732891082763672</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 366: 0.08704856038093567</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 367: 0.08765564113855362</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 368: 0.08696923404932022</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 369: 0.08649873733520508</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 370: 0.08676613122224808</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 371: 0.08599219471216202</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 372: 0.08565033972263336</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 373: 0.08618329465389252</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 374: 0.08559156954288483</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 375: 0.08509930223226547</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 376: 0.08543801307678223</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 377: 0.084554523229599</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 378: 0.08425222337245941</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 379: 0.08496475219726562</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 380: 0.08428442478179932</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 381: 0.08389458060264587</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 382: 0.08416417241096497</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 383: 0.08331726491451263</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 384: 0.08304726332426071</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 385: 0.0837259590625763</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 386: 0.08301664143800735</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 387: 0.08279375731945038</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 388: 0.08285657316446304</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 389: 0.0822003185749054</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 390: 0.08189017325639725</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 391: 0.08244460821151733</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 392: 0.08176209777593613</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 393: 0.08143384009599686</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 394: 0.08153267949819565</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 395: 0.08074252307415009</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 396: 0.0804641842842102</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 397: 0.08112648874521255</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 398: 0.0804068073630333</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 399: 0.08000007271766663</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 400: 0.08030638843774796</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 401: 0.07946185022592545</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 402: 0.07926557213068008</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 403: 0.07995376735925674</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 404: 0.07914069294929504</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 405: 0.07901032269001007</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 406: 0.07910943776369095</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 407: 0.07840055227279663</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 408: 0.07814037799835205</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 409: 0.07874786853790283</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 410: 0.07819069921970367</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 411: 0.07780887931585312</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 412: 0.07802116870880127</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 413: 0.0772867277264595</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 414: 0.07709880918264389</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 415: 0.0776868537068367</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 416: 0.07716330885887146</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 417: 0.07688125967979431</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 418: 0.07698465138673782</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 419: 0.0762372612953186</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 420: 0.07603802531957626</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 421: 0.07675285637378693</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 422: 0.07623977214097977</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 423: 0.07567108422517776</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 424: 0.07615751028060913</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 425: 0.07526733726263046</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 426: 0.07509555667638779</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 427: 0.07569493353366852</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 428: 0.07537294924259186</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 429: 0.07467805594205856</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 430: 0.07528648525476456</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 431: 0.07435967028141022</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 432: 0.07422596961259842</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 433: 0.07503972947597504</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 434: 0.07434249669313431</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 435: 0.07409335672855377</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 436: 0.07420685887336731</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 437: 0.0735834538936615</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 438: 0.07333341240882874</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 439: 0.07391082495450974</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 440: 0.07348911464214325</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 441: 0.07308389991521835</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 442: 0.07328886538743973</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 443: 0.0725550651550293</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 444: 0.07240220904350281</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 445: 0.07308465242385864</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 446: 0.07288312911987305</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 447: 0.0722663402557373</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 448: 0.07264856994152069</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 449: 0.07182618230581284</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 450: 0.07167533785104752</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 451: 0.07241341471672058</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 452: 0.07208056002855301</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 453: 0.07154601812362671</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 454: 0.07190731167793274</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 455: 0.0710812360048294</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 456: 0.07096673548221588</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 457: 0.0718337818980217</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 458: 0.07134897261857986</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 459: 0.07083813846111298</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 460: 0.07124733179807663</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 461: 0.0705094262957573</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 462: 0.07036501169204712</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 463: 0.07111788541078568</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 464: 0.07069509476423264</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 465: 0.07026039808988571</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 466: 0.07056906819343567</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 467: 0.06981150805950165</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 468: 0.06967213749885559</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 469: 0.0704450011253357</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 470: 0.07002224773168564</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 471: 0.06954890489578247</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 472: 0.07001929730176926</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 473: 0.06918215751647949</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 474: 0.06905678659677505</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 475: 0.06994140148162842</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 476: 0.06957031041383743</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 477: 0.06890591233968735</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 478: 0.06942413747310638</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 479: 0.068662129342556</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 480: 0.0685315951704979</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 481: 0.06919320672750473</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 482: 0.06884051114320755</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 483: 0.06852498650550842</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 484: 0.06881336867809296</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 485: 0.0681278333067894</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 486: 0.06801153719425201</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 487: 0.0688665509223938</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 488: 0.06848578155040741</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 489: 0.0680362805724144</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 490: 0.0685308426618576</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 491: 0.06770123541355133</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 492: 0.06760372221469879</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 493: 0.06856502592563629</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 494: 0.0679614394903183</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 495: 0.0675961971282959</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 496: 0.06795072555541992</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 497: 0.06731095910072327</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 498: 0.06714644283056259</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 499: 0.06786693632602692</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 500: 0.06758256256580353</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 501: 0.06698315590620041</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 502: 0.06747950613498688</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 503: 0.06655343621969223</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 504: 0.06652842462062836</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 505: 0.06745205074548721</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 506: 0.0668550580739975</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 507: 0.06666403263807297</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 508: 0.06683854013681412</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 509: 0.06626935303211212</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 510: 0.06613652408123016</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 511: 0.06672576069831848</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 512: 0.0666651502251625</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 513: 0.06582488119602203</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 514: 0.06652247160673141</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 515: 0.06558185815811157</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 516: 0.0655498206615448</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 517: 0.06624851375818253</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 518: 0.06601088494062424</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 519: 0.06545697897672653</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 520: 0.0659414529800415</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 521: 0.06516807526350021</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 522: 0.06501934677362442</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 523: 0.06574487686157227</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 524: 0.06553597748279572</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 525: 0.06504649668931961</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 526: 0.06540416181087494</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 527: 0.06479271501302719</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 528: 0.06469936668872833</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 529: 0.0654490739107132</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 530: 0.06509881466627121</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 531: 0.06460769474506378</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 532: 0.06506450474262238</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 533: 0.06425388902425766</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 534: 0.06419297307729721</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 535: 0.06507144123315811</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 536: 0.06475593149662018</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 537: 0.0640476867556572</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 538: 0.06452148407697678</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 539: 0.063988097012043</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 540: 0.06390102207660675</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 541: 0.06427431106567383</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 542: 0.06461699306964874</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 543: 0.06366197764873505</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 544: 0.06439769268035889</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 545: 0.06354749947786331</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 546: 0.06346575170755386</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 547: 0.06415951251983643</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 548: 0.06416907906532288</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 549: 0.06350232660770416</span>
/home/runner/work/lightwood/lightwood/lightwood/mixer/neural.py:335: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler(&#39;cuda&#39;, args...)` instead.
  scaler = GradScaler()
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 1: 0.03389815576374531</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 2: 0.033698095567524435</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 3: 0.0372611828148365</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 4: 0.0382374182343483</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 5: 0.03677316829562187</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 6: 0.04194173291325569</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 7: 0.04046095162630081</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `fit_mixer` runtime: 4.29 seconds</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Started fitting XGBoost model</span>
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0]     validation_0-mlogloss:0.85953
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-3286:A single GBM iteration takes 0.1 seconds</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Training XGBoost with 131 iterations given 16.484926993846894 seconds constraint</span>
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[0]     validation_0-mlogloss:0.85953
[1]     validation_0-mlogloss:0.58684
[2]     validation_0-mlogloss:0.41458
[3]     validation_0-mlogloss:0.29824
[4]     validation_0-mlogloss:0.21700
[5]     validation_0-mlogloss:0.15916
[6]     validation_0-mlogloss:0.11747
[7]     validation_0-mlogloss:0.08717
[8]     validation_0-mlogloss:0.06502
[9]     validation_0-mlogloss:0.04876
[10]    validation_0-mlogloss:0.03678
[11]    validation_0-mlogloss:0.02793
[12]    validation_0-mlogloss:0.02138
[13]    validation_0-mlogloss:0.01651
[14]    validation_0-mlogloss:0.01288
[15]    validation_0-mlogloss:0.01017
[16]    validation_0-mlogloss:0.00813
[17]    validation_0-mlogloss:0.00659
[18]    validation_0-mlogloss:0.00542
[19]    validation_0-mlogloss:0.00455
[20]    validation_0-mlogloss:0.00392
[21]    validation_0-mlogloss:0.00351
[22]    validation_0-mlogloss:0.00320
[23]    validation_0-mlogloss:0.00296
[24]    validation_0-mlogloss:0.00296
[25]    validation_0-mlogloss:0.00295
[26]    validation_0-mlogloss:0.00295
[27]    validation_0-mlogloss:0.00295
[28]    validation_0-mlogloss:0.00296
[29]    validation_0-mlogloss:0.00296
[30]    validation_0-mlogloss:0.00297
[31]    validation_0-mlogloss:0.00298
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
<span class="ansi-green-fg">INFO:lightwood-3286:XGBoost mixer does not have a `partial_fit` implementation</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `fit_mixer` runtime: 0.05 seconds</span>
<span class="ansi-yellow-fg">WARNING:dataprep_ml-3286:Exception: Unspported categorical type for regression when training mixer: &lt;lightwood.mixer.regression.Regression object at 0x7f288cd46e80&gt;</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Started fitting RandomForest model</span>
<span class="ansi-green-fg">INFO:lightwood-3286:RandomForest based correlation of (train data): 1.0</span>
<span class="ansi-green-fg">INFO:lightwood-3286:RandomForest based correlation of (dev data): 1.0</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `fit_mixer` runtime: 0.1 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Ensembling the mixer</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Mixer: Neural got accuracy: 0.922</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Mixer: XGBoostMixer got accuracy: 1.0</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Mixer: RandomForest got accuracy: 1.0</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Picked best mixer: RandomForest</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `fit` runtime: 4.48 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 7/8] - Ensemble analysis</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Analyzing the ensemble of mixers</span>
<span class="ansi-green-fg">INFO:lightwood-3286:The block ICP is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-3286:The block ConfStats is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-3286:The block AccStats is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-3286:The block PermutationFeatureImportance is now running its analyze() method</span>
<span class="ansi-green-fg">INFO:lightwood-3286:[PFI] Using a random sample (1000 rows out of 22).</span>
<span class="ansi-green-fg">INFO:lightwood-3286:[PFI] Set to consider first 10 columns out of 6: [&#39;Population&#39;, &#39;Area (sq. mi.)&#39;, &#39;Pop. Density &#39;, &#39;GDP ($ per capita)&#39;, &#39;Literacy (%)&#39;, &#39;Infant mortality &#39;].</span>
<span class="ansi-green-fg">INFO:lightwood-3286:The block ModelCorrelationHeatmap is now running its analyze() method</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `analyze_ensemble` runtime: 0.21 seconds</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:[Learn phase 8/8] - Adjustment on validation requested</span>
<span class="ansi-green-fg">INFO:dataprep_ml-3286:Updating the mixers</span>
/home/runner/work/lightwood/lightwood/lightwood/mixer/neural.py:335: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler(&#39;cuda&#39;, args...)` instead.
  scaler = GradScaler()
/opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 1: 0.033697554686417185</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 2: 0.033981192080924906</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 3: 0.037426896315688886</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 4: 0.04428015494098266</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 5: 0.061086510928968586</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 6: 0.03466159128583968</span>
<span class="ansi-green-fg">INFO:lightwood-3286:Loss @ epoch 7: 0.03769115870818496</span>
<span class="ansi-green-fg">INFO:lightwood-3286:XGBoost mixer does not have a `partial_fit` implementation</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `adjust` runtime: 0.06 seconds</span>
<span class="ansi-white-fg">DEBUG:lightwood-3286: `learn` runtime: 4.86 seconds</span>
</pre></div></div>
</div>
<p>Finally, we can visualize the mixer correlation matrix:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">mc</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">runtime_analyzer</span><span class="p">[</span><span class="s1">&#39;mixer_correlation&#39;</span><span class="p">]</span>  <span class="c1"># newly produced insight</span>

<span class="n">mixer_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">predictor</span><span class="o">.</span><span class="n">ensemble</span><span class="o">.</span><span class="n">mixers</span><span class="p">]</span>

<span class="c1"># plotting code</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mc</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;seismic&#39;</span><span class="p">)</span>

<span class="c1"># set ticks</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mc</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># set tick labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)</span>

<span class="c1"># show cell values</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mixer_names</span><span class="p">)):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/tutorials_custom_explainer_custom_explainer_20_0.png" src="../../_images/tutorials_custom_explainer_custom_explainer_20_0.png" />
</div>
</div>
<p>Nice! We’ve just added an additional piece of insight regarding the predictor that Lightwood came up with for the task of predicting the Human Development Index of any given country.</p>
<p>What this matrix is telling us is whether predictions of each pair of the mixers stored in the ensemble have a high correlation or not.</p>
<p>This is, of course, a very simple example, but it shows the convenience of such an abstraction within the broader pipeline that Lightwood automates.</p>
<p>For more complex examples, you can check out any of the three core analysis blocks that we use:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">lightwood.analysis.nc.calibrate.ICP</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lightwood.analysis.helpers.acc_stats.AccStats</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lightwood.analysis.helpers.feature_importance.PermutationFeatureImportance</span></code></p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017-2025, MindsDB.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>